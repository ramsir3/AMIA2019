{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafn = 'HOUR_00001.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('~/Projects/Research/data/processed/', datafn), na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where all features=nan\n",
    "row_nan_bool = np.logical_not(np.all(np.isnan(data.iloc[:,5:-1]), axis=1))\n",
    "data2 = data[row_nan_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#drop columns where all rows=nan\n",
    "check_nan = data2.isna().sum()\n",
    "data2.drop(labels=check_nan[(check_nan == data2.shape[0])].keys(), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.iloc[:,3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate Kruskal-Wallis H-test for each feature\n",
    "dfs_by_class = [data3.loc[data2['STAGE'] == c] for c in [0,1,2,3]]\n",
    "kruskals = {}\n",
    "for col in data3:\n",
    "    kruskals[col] = sp.stats.kruskal(*[np.asarray(c[col].dropna()) for c in dfs_by_class])[1]\n",
    "           \n",
    "data2 = data2[[\"SUBJECT_ID\",\"HADM_ID\",\"TSTAGE\"]+[k for k, v in kruskals.items() if v > 0.05]+[\"STAGE\"]]\n",
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanImputeData = data2.fillna(data.mean())\n",
    "print(meanImputeData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate VIFs\n",
    "done = -1\n",
    "while done != 0:\n",
    "    vifs = {}\n",
    "    for i, n in enumerate(meanImputeData):\n",
    "        if i in range(3,meanImputeData.shape[1]):\n",
    "            vifs[n] = variance_inflation_factor(np.asarray(meanImputeData), i)\n",
    "\n",
    "    drop_vifs = [k for k,v in vifs.items() if v >= 5 or np.isnan(v)]\n",
    "    print(drop_vifs)\n",
    "    meanImputeData.drop(labels=drop_vifs, axis=1, inplace=True)\n",
    "    done = len(drop_vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meanImputeData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanImputeDataArray = meanImputeData.values[:, 3:]\n",
    "x = meanImputeDataArray[:, :-1]\n",
    "y = meanImputeDataArray[:, -1]\n",
    "x = normalize(x, axis=0)\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit(y.reshape(-1, 1))\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "ohe_y_train = ohe.transform(y_train.reshape(-1,1))\n",
    "ohe_y_test = ohe.transform(y_test.reshape(-1,1))\n",
    "print(ohe_y_train.shape)\n",
    "print(ohe_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(model, x, y, x_test, y_test):\n",
    "    m = OneVsRestClassifier(model)\n",
    "    m.fit(x,y)\n",
    "    y_predict = m.predict(x_test)\n",
    "    \n",
    "    return m, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMetrics(y_test, y_predict):\n",
    "    TP = {}\n",
    "    FP = {}\n",
    "    TN = {}\n",
    "    FN = {}\n",
    "\n",
    "    for i in range(y_predict.shape[1]): \n",
    "        TP[i] = np.sum(np.logical_and(y_predict[:,i]==1,y_test[:,i]==1))\n",
    "        FP[i] = np.sum(np.logical_and(y_predict[:,i]==1,y_test[:,i]!=y_predict[:,i]))\n",
    "        TN[i] = np.sum(np.logical_and(y_predict[:,i]==0,y_test[:,i]==0))\n",
    "        FN[i] = np.sum(np.logical_and(y_predict[:,i]==0,y_test[:,i]!=y_predict[:,i]))\n",
    "    \n",
    "    out = {}\n",
    "    for i in range(y_predict.shape[1]): \n",
    "        out[i] = [TP[i], FP[i], TN[i], FN[i]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_auc(y_test, y_score):\n",
    "    n_classes = 4\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = skm.roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = skm.auc(fpr[i], tpr[i])\n",
    "    roc_auc['avg'] = sum(roc_auc.values())/n_classes\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'SVC': SVC(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'GaussianProcessClassifier': GaussianProcessClassifier(), # this one was giving me an out of memory error\n",
    "    'MLPClassifier': MLPClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawScores = {}\n",
    "accScores = {}\n",
    "for n, m in models.items():\n",
    "    print(n)\n",
    "    m, y_predict = runModel(m, x_train, ohe_y_train, x_test, ohe_y_test)\n",
    "    accScores[n] = skm.accuracy_score(ohe_y_test, y_predict)\n",
    "    rawScores[n] = runMetrics(ohe_y_test, y_predict)\n",
    "    if n in {'SVC', 'SGDClassifier', 'GradientBoostingClassifier'}:\n",
    "        aucs = multiclass_auc(ohe_y_test, m.decision_function(x_test))\n",
    "    else:\n",
    "        aucs = multiclass_auc(ohe_y_test, m.predict_proba(x_test))\n",
    "        \n",
    "    for k in rawScores[n].keys():\n",
    "        rawScores[n][k].append(aucs[k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RAW_OUTPUT_\"+datafn, 'w') as fout:\n",
    "    header = ',' + ',,,,'.join(\"CLASS %d (%d)\" % (c, np.sum(ohe_y_test[:,c])) for c in [0,1,2,3]) + '\\n'\n",
    "    header+= 'MODEL,' + ','.join(\"TP,FP,TN,FN\" for c in [0,1,2,3]) + '\\n'\n",
    "    fout.write(header)\n",
    "    for m in rawScores:\n",
    "        fout.write(m+',')\n",
    "        for c in rawScores[m]:\n",
    "            for i in rawScores[m][c][:-1]:\n",
    "                fout.write(str(i)+',')\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC,PPV,NPV,SEN,SPE,F1\n",
    "# TP,FP,TN,FN\n",
    "def calcScores(rs):\n",
    "#     print('?',rs)\n",
    "    auc = rs[4]\n",
    "    ppv = rs[0]/(rs[0]+rs[1])\n",
    "    npv = rs[2]/(rs[2]+rs[3])\n",
    "    sen = rs[0]/(rs[0]+rs[3])\n",
    "    spe = rs[2]/(rs[0]+rs[1])\n",
    "    f1  = (2*rs[0])/((2*rs[0])+rs[1]+rs[3])\n",
    "    return (auc,ppv,npv,sen,spe,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"OUTPUT_\"+datafn, 'w') as fout:\n",
    "    header = ',AVG (%d),,,,,,' % ohe_y_test.shape[0] + ',,,,,,'.join(\"CLASS %d (%d)\" % (c, np.sum(ohe_y_test[:,c])) for c in [0,1,2,3]) + '\\n'\n",
    "    header+= 'MODEL,ACC,AUC,PPV,NPV,SEN,SPE,F1,' + ','.join(\"AUC,PPV,NPV,SEN,SPE,F1\" for c in [0,1,2,3]) + '\\n'\n",
    "    fout.write(header)\n",
    "    for m in rawScores:\n",
    "        calcedScores = [calcScores(rawScores[m][k]) for k in rawScores[m]]\n",
    "        print(calcedScores)\n",
    "        avgScores = [0 for i in calcedScores[0]]\n",
    "        print(avgScores)\n",
    "        for i, c in enumerate(calcedScores):\n",
    "            for j, _ in enumerate(c):\n",
    "                avgScores[j] += calcedScores[i][j]\n",
    "        for i, v in enumerate(avgScores):\n",
    "            avgScores[i] = v/len(calcedScores)       \n",
    "        \n",
    "        fout.write(m+',')\n",
    "        fout.write(str(accScores[m])+',')\n",
    "        for v in avgScores:\n",
    "            fout.write(str(v)+',')\n",
    "        for c in calcedScores:\n",
    "            for i in c:\n",
    "                fout.write(str(i)+',')\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
